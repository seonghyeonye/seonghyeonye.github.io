<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Seonghyeon Ye</title>
    <link rel="icon" type="image/png" href="kaist.ico">
    <style>
      body {
          font-family: Arial, sans-serif;
          background-color: #f4f4f4;
          margin: 0;
          padding: 20px;
      }

      /* Container for centering the content and limiting width */
      .container {
          width: 50%;  /* Limit the width to 50% on larger screens */
          margin: 0 auto;  /* Center the container */
          background-color: #fff;
          padding: 50px;
          border-radius: 10px;
          box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      }

      .header-container {
          display: flex;
          align-items: center;
          justify-content: space-between;
      }

      .post-title {
          font-size: 2.5em;
          margin-bottom: 10px;
      }

      .font-weight-bold {
          font-weight: bold;
      }

      .desc {
          font-size: 1.2em;
          color: #333;
      }

      .desc i {
          color: #555;
      }

      .post-header a {
          color: #1a73e8;
          text-decoration: none;
          margin: 0 1px;
      }

      .post-header a:hover {
          text-decoration: underline;
      }

      .profile-image {
          flex-shrink: 0;
          margin-left: 40px;
      }

      .profile-image img {
          width: 150px;
          height: 150px;
          border-radius: 50%;
          box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      }

      hr {
          border: none;
          border-top: 1px solid #ddd;
          margin: 20px 0;
      }

      .clearfix {
          margin-top: 20px;
      }

      .clearfix p {
          font-size: 1.1em;
          line-height: 1.6;
      }

      .clearfix a {
          color: #1a73e8;
          text-decoration: none;
      }

      .clearfix a:hover {
          text-decoration: underline;
      }

      /* Latest News Section */
      .publication-section {
          background-color: #fff;
          padding: 30px;
          border-radius: 10px;
          box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
          margin-top: 20px;
      }

      .publication-section h2 {
          font-size: 2em;
          margin-bottom: 10px;
      }

      .news-list {
          list-style-type: none;
          padding: 0;
      }

      .news-list li {
          font-size: 1.1em;
          line-height: 1.6;
          margin-bottom: 10px;
      }

      /* Make publication links light blue */
      .publication-section a {
          color: #1a73e8;  /* Light blue color for links */
          text-decoration: none;
      }

      .publication-section a:hover {
          text-decoration: underline;
      }

      .spotlight {
          background-color: #ffd700; /* gold */
          padding: 2px 5px;
          border-radius: 5px;
          font-weight: bold;
          color: #333;
      }

      /* Work Experience and Education styles similar to Publication */
      .education-section, .work-experience-section {
          background-color: #fff;
          padding: 30px;
          border-radius: 10px;
          box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
          margin-top: 20px;
      }

      .education-section h2, .work-experience-section h2 {
          font-size: 2em;
          margin-bottom: 10px;
      }

      /* Apply the same font size and line spacing as publications */
      .education-list li, .work-list li {
          font-size: 1.1em;
          line-height: 1.6;
          margin-bottom: 10px;
      }

      /* Media queries for responsiveness */
      @media (max-width: 768px) {
          .container {
              width: 100%; /* Take full width on smaller screens */
              padding: 20px;
          }

          .header-container {
              flex-direction: column;
              align-items: center;
              text-align: center;
          }

          .profile-image {
              margin-left: 0; /* Remove left margin for centered alignment */
              margin-top: 20px;
          }
      }
  </style>
</head>

<body>
  <div class="container">
      <div class="header-container">
          <header class="post-header">
              <h1 class="post-title">
                  <span class="font-weight-bold">Seonghyeon Ye</span>
              </h1>
              <p class="desc"><i><b>KAIST Graduate school of AI</b></i></p>
              <p>
                  <a href="mailto:seonghyeon.ye@kaist.ac.kr">[Mail]</a>
                  <a href="https://github.com/seonghyeonye" target="_blank">[GitHub]</a>
                  <a href="https://scholar.google.com/citations?user=JfGGjBoAAAAJ&hl=en&oi=ao" target="_blank">[Google Scholar]</a>
                  <a href="https://twitter.com/SeonghyeonYe" target="_blank">[X]</a>
              </p>
              <hr>
              <div class="clearfix">
                  <p>Hello, I am a second year Ph.D student in KAIST Graduate school of AI, advised by <a href="https://seominjoon.github.io/">Minjoon Seo</a> and <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>. I am currently interested in building robotic foundation models using internet-scale video data.</p>
              </div>
          </header>

          <div class="profile-image">
              <img src="SeonghyeonYe.JPG" alt="Seonghyeon Ye's Photo">
          </div>
      </div>

    <!-- Latest News Section -->
    <div class="publication-section">
        <h2>Publications</h2>
        <hr>
        <h3>2024</h3>
        <ul class="news-list">
            <li>
                How Do Large Language Models Acquire Factual Knowledge During Pretraining?<br>
                Hoyeon Chang, Jinho Park, <ins>Seonghyeon Ye</ins>, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo<br>
                <i>NeurIPS 2024</i> 
                <br><a href="https://arxiv.org/abs/2406.11813">[paper]</a> <a href="https://github.com/kaistAI/factual-knowledge-acquisition">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Instruction Matters, a Simple yet Effective Task Selection Approach in Instruction Tuning for Specific Tasks<br>
                Changho Lee, Janghoon Han, <ins>Seonghyeon Ye</ins>, Stanley Jungkyu Choi, Honglak Lee, Kyunghoon Bae<br>
                <i>EMNLP 2024</i> 
                <br><a href="https://arxiv.org/abs/2406.11813">[paper]</a> <a href="https://github.com/CHLee0801/INSTA">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards<br>
                Hyeonbin Hwang, Doyoung Kim, Seungone Kim, <ins>Seonghyeon Ye</ins>, Minjoon Seo<br>
                <i>EMNLP 2024 Findings</i> 
                <br><a href="https://arxiv.org/abs/2404.10346">[paper]</a> <a href="https://github.com/hbin0701/Self-Explore">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets<br>
                <ins>Seonghyeon Ye<sup>*</sup></b></ins>, Doyoung Kim<sup>*</sup>, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo<br>
                <i>ICLR 2024</i> 
                <span class="spotlight">Spotlight</span>
                <br><a href="https://arxiv.org/abs/2307.10928">[paper]</a> <a href="https://github.com/kaistAI/FLASK">[code]</a></p> 
            </li>
            <!-- Add more papers for 2024 -->
        </ul>
        <ul class="news-list">
            <li>
                Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis<br>
                Sohee Yang, Jonghyeon Kim, Joel Jang, <ins>Seonghyeon Ye</ins>, Hyunji Lee, Minjoon Seo<br>
                <i>TACL 2024</i>
                <br><a href="https://arxiv.org/abs/2305.14877">[paper]</a> <a href="https://github.com/soheeyang/unified-prompt-selection">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following<br>
                <ins>Seonghyeon Ye</ins>, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, Minjoon Seo<br>
                <i>AAAI 2024</i>
                <br><a href="https://arxiv.org/abs/2302.14691">[paper]</a> <a href="https://github.com/seonghyeonye/icil">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models<br>
                Yujin Kim, Jaehong Yoon, <ins>Seonghyeon Ye</ins>, Sung Ju Hwang, Se-young Yun<br>
                <i>NAACL 2024</i>
                <br><a href="https://arxiv.org/abs/2311.08106">[paper]</a>
            </li>
        </ul>

        <hr>
        <h3>2023</h3>
        
        <ul class="news-list">
            <li>
                The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-tuning<br>
                Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, <ins>Seonghyeon Ye</ins>, Jamin Shin, Minjoon Seo<br>
                <i>EMNLP 2023</i>
                <br><a href="https://arxiv.org/abs/2305.14045">[paper]</a> <a href="https://github.com/kaist-lklab/CoT-Collection">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt<br>
                <ins>Seonghyeon Ye</ins>, Joel Jang, Doyoung Kim, Yongrae Jo, Minjoon Seo<br>
                <i>EMNLP 2023 Findings</i>
                <br><a href="https://arxiv.org/abs/2210.03029">[paper]</a> <a href="https://github.com/seonghyeonye/RoSPr">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Exploring the Benefits of Training Expert Language Models over Instruction Tuning<br>
                Joel Jang, Seungone Kim, <ins>Seonghyeon Ye</ins>, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, Minjoon Seo<br>
                <i>ICML 2023</i>
                <br><a href="https://arxiv.org/abs/2302.03202">[paper]</a> <a href="https://github.com/joeljang/ELM">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners<br>
                <ins>Seonghyeon Ye</ins>, Doyoung Kim, Joel Jang, Joongbo Shin, Minjoon Seo<br>
                <i>ICLR 2023</i>
                <br><a href="https://arxiv.org/abs/2210.02969">[paper]</a> <a href="https://github.com/seonghyeonye/Flipped-Learning">[code]</a></p> 
            </li>
        </ul>
        <ul class="news-list">
            <li>
                SelFee: Iterative Self-Revising LLM Empowered by Self-Feedback Generation<br>
                <ins>Seonghyeon Ye<sup>*</sup></b></ins>, Yongrae Jo<sup>*</sup>, Doyoung Kim<sup>*</sup>, Sungdong Kim, Hyeonbin Hwang, Minjoon Seo<br>
                <i>Blog post</i>
                <br><a href="https://kaistai.github.io/SelFee/">[blog]</a> <a href="https://github.com/kaistAI/SelFee">[code]</a></p> 
            </li>
        </ul>
        <hr>
        <h3>2022</h3>
        <ul class="news-list">
            <li>
                Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts<br>
                Joel Jang<sup>*</sup>, <ins>Seonghyeon Ye</ins><sup>*</sup>, Minjoon Seo<br>
                <i>Transfer Learning for NLP Workshop @ NeurIPS 2022</i>
                <br><a href="https://arxiv.org/abs/2209.12711">[paper]</a> <a href="https://github.com/joeljang/negated-prompts-for-llms">[code]</a></p>
            </li>
        </ul>
        <ul class="news-list">
            <li>
                TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models<br>
                Joel Jang<sup>*</sup>, <ins>Seonghyeon Ye</ins><sup>*</sup>, Chango Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Minjoon Seo<br>
                <i>EMNLP 2022</i>
                <br><a href="https://arxiv.org/abs/2204.14211">[paper]</a> <a href="https://github.com/joeljang/temporalwiki">[code]</a></p>
            </li>
        </ul>
        <ul class="news-list">
            <li>
                Towards Continual Knowledge Learning of Language Models<br>
                Joel Jang, <ins>Seonghyeon Ye</ins>, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo<br>
                <i>ICLR 2022</i>
                <br><a href="https://arxiv.org/abs/2110.03215">[paper]</a> <a href="https://github.com/joeljang/continual-knowledge-learning">[code]</a></p>  
            </li>
        </ul>
        <hr>
        <h3>2021</h3>
        <ul class="news-list">
            <li>
                Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning<br>
                <ins>Seonghyeon Ye</ins>, Jiseon Kim, Alice Oh<br>
                <i>EMNLP 2021 (short)</i>
                <br><a href="https://arxiv.org/abs/2109.05941">[paper]</a> <a href="https://github.com/seonghyeonye/EfficientCL">[code]</a></p>
            </li>  
        </ul>
        <ul class="news-list">
            <li>
                Dimensional Emotion Detection from Categorical Emotion<br>
                Sungjoon Park, Jiseon Kim, <ins>Seonghyeon Ye</ins>, Jaeyeol Jeon, Hee Young Park, Alice Oh<br>
                <i>EMNLP 2021</i>
                <br><a href="https://arxiv.org/abs/1911.02499">[paper]</a> <a href="https://github.com/SungjoonPark/EmotionDetection">[code]</a></p>
            </li>
        </ul>
    </div>
    <!-- Education Section -->
    <div class="education-section">
        <h2>Education</h2>
        <hr>

        <ul class="education-list">
            <li>
                <strong>KAIST AI</strong><br>
                M.S. & Ph.D. in Artificial Intelligence, <i>2022 - Present</i><br>
                Advisor: Minjoon Seo, Kimin Lee
            </li>
            <li>
                <strong>KAIST CS</strong><br>
                B.S. in Computer Science, <i>2017 - 2021</i><br>
                Advisor: Alice Oh, Jong C. Park
            </li>
        </ul>
    </div>
    <div class="work-experience-section">
        <h2>Work Experience</h2>
        <hr>
    
        <ul class="work-list">
            <li>
                <strong>Microsoft Research</strong><br>
                Research Intern, <i>June 2024 - September 2024</i><br>
                Working with <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a>, <a href="https://jwyang.github.io/">Jianwei Yang</a>, and <a href="https://scholar.google.com/citations?user=u1CNjgwAAAAJ">Baolin Peng</a>
            </li>
            <li>
                <strong>LG AI Research</strong><br>
                Research Intern, <i>July 2022 - Mar 2023</i><br>
                Working with <a href="https://scholar.google.co.kr/citations?user=xzJSvJcAAAAJ&hl=en">Joongbo Shin</a>
            </li>
            <!-- Add more work experiences if needed -->
        </ul>
    </div>
  </div>

</body>
</html>
